{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor, ToPILImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Энкодер для автокодировщика на основе U-Net.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels, features=64):\n",
    "        super(UNetEncoder, self).__init__()\n",
    "        self.enc1 = self.contract_block(input_channels, features)\n",
    "        self.enc2 = self.contract_block(features, features * 2)\n",
    "        self.enc3 = self.contract_block(features * 2, features * 4)\n",
    "        self.enc4 = self.contract_block(features * 4, features * 8)\n",
    "\n",
    "    def contract_block(self, in_channels, out_channels, kernel_size=3):\n",
    "        contract = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        return contract\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.enc1(x)\n",
    "        x2 = self.enc2(x1)\n",
    "        x3 = self.enc3(x2)\n",
    "        x4 = self.enc4(x3)\n",
    "        return x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Декодер для автокодировщика на основе U-Net.\n",
    "    \"\"\"\n",
    "    def __init__(self, output_channels, features=64):\n",
    "        super(UNetDecoder, self).__init__()\n",
    "        self.dec1 = self.expand_block(features * 8, features * 4)\n",
    "        self.dec2 = self.expand_block(features * 4, features * 2)\n",
    "        self.dec3 = self.expand_block(features * 2, features)\n",
    "        self.final = nn.Conv2d(features, output_channels, kernel_size=1)\n",
    "\n",
    "    def expand_block(self, in_channels, out_channels, kernel_size=3):\n",
    "        expand = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ConvTranspose2d(out_channels, out_channels, kernel_size=2, stride=2)\n",
    "        )\n",
    "        return expand\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dec1(x)\n",
    "        x = self.dec2(x)\n",
    "        x = self.dec3(x)\n",
    "        x = self.final(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Автокодировщик, сочетающий энкодер и декодер U-Net.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels, output_channels):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = UNetEncoder(input_channels)\n",
    "        self.decoder = UNetDecoder(output_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предобработка данных\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((640, 640)),\n",
    "])\n",
    "\n",
    "# Загрузка данных\n",
    "train_data = datasets.ImageFolder(root='train', transform=transform)\n",
    "val_data = datasets.ImageFolder(root='val', transform=transform)\n",
    "test_data = datasets.ImageFolder(root='test', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "# Инициализация модели, функции потерь и оптимизатора\n",
    "model = Autoencoder(input_channels=3, output_channels=3)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Обучение модели\n",
    "def train_model(model, criterion, optimizer, num_epochs=25):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for data in train_loader:\n",
    "            inputs, _ = data\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "\n",
    "        # Валидация\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                inputs, _ = data\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, inputs)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "train_model(model, criterion, optimizer, num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для инференса и сохранения изображений\n",
    "def infer_and_save(input_dir, output_dir, model):\n",
    "    # Создаем директорию для вывода, если она не существует\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    model.eval()  # Переключаем модель в режим инференса\n",
    "\n",
    "    for image_name in os.listdir(input_dir):\n",
    "        # Загружаем и обрабатываем изображение\n",
    "        image_path = os.path.join(input_dir, image_name)\n",
    "        image = Image.open(image_path)\n",
    "        input_image = ToTensor()(image).unsqueeze(0)  # Добавляем размерность batch\n",
    "\n",
    "        # Применяем модель\n",
    "        with torch.no_grad():\n",
    "            output_image = model(input_image)\n",
    "\n",
    "        # Преобразуем результат в PIL Image и сохраняем\n",
    "        output_image = ToPILImage()(output_image.squeeze(0))\n",
    "        output_image.save(os.path.join(output_dir, image_name))\n",
    "\n",
    "# Загрузка обученной модели (предполагается, что модель уже сохранена)\n",
    "model = Autoencoder(input_channels=3, output_channels=3)\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "\n",
    "# Инференс и сохранение результатов\n",
    "infer_and_save('input_images', 'output_images', model)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
